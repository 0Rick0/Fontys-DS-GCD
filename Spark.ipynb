{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spark and Python\n",
    "Spark is another framework ontop of HDFS/Hadoop.\n",
    "\n",
    "It gives an api compatible with many languages, includig Python.\n",
    "\n",
    "In this notebook I will give some examples based on an NGINX like access log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext('local', 'ipynb Example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "file = sc.textFile('hdfs://localhost:8020/user/root/GCD-Week-6-access_log.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A regex for matching the nginx log line.\n",
    "# The only problem with this approach is that it does not always match every line\n",
    "reg = re.compile('(?P<ipaddress>\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}) - - \\[(?P<dateandtime>\\d{2}\\/[a-zA-Z]{3}\\/\\d{4}:\\d{2}:\\d{2}:\\d{2} (\\+|\\-)\\d{4})\\] ((\\\"(?P<method>[A-Z]+) )(?P<url>.+) (HTTP\\/1\\.1\\\")) (?P<statuscode>\\d{3}) (?P<bytessent>\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.99.99.58 - - [17/Jul/2011:10:51:29 -0700] \"GET /favicon.ico HTTP/1.1\" 200 1406']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.top(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the amount of requests per path or address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First parse the line\n",
    "# Then get the url component of the line if the line was successfully parsed\n",
    "# Then map it to one, e.g. ('/', 1)\n",
    "# Then reduce it to count them\n",
    "counts = file.map(lambda line: reg.match(line))\\\n",
    "    .map(lambda group: group.group('url') if group else None)\\\n",
    "    .map(lambda url: (url, 1))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "actual_counts = dict(counts.collect())\n",
    "# For example:\n",
    "# ‘/assets/js/the-associates.js’\n",
    "actual_counts['/assets/js/the-associates.js']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is very similar with the source ip:\n",
    "counts_ip = file.map(lambda line: reg.match(line))\\\n",
    "    .map(lambda group: group.group('ipaddress') if group else None)\\\n",
    "    .map(lambda ip: (ip, 1))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "actual_counts_ip = dict(counts_ip.collect())\n",
    "\n",
    "# for example 10.99.99.186\"\n",
    "actual_counts_ip['10.99.99.186']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None: 636479\n/assets/css/combined.css: 103709\n/assets/js/javascript_combined.js: 99176\n/: 92372\n/assets/img/home-logo.png: 84132\n/assets/css/printstyles.css: 83268\n/images/filmpics/0000/3695/Pelican_Blood_2D_Pack.jpg: 83105\n/favicon.ico: 63228\n/robots.txt: 38732\n/images/filmpics/0000/3139/SBX476_Vanquisher_2d.jpg: 35633\n/assets/img/search-button.gif: 31834\n/images/filmmediablock/290/Harpoon_2d.JPG: 30793\n/assets/img/play_icon.png: 30241\n/images/filmpics/0000/1421/RagingPhoenix_2DSleeve.jpeg: 27549\n/assets/img/x.gif: 25081\n/release-schedule/: 24620\n/search/: 22744\n/assets/img/release-schedule-logo.png: 20518\n/release-schedule: 17740\n/assets/img/banner/ten-years-banner-white.jpg: 15877\n"
     ]
    }
   ],
   "source": [
    "# Ordering is also quite easy to do in spark. For instance for the most commonly requested file:\n",
    "counts = file.map(lambda line: reg.match(line))\\\n",
    "    .map(lambda group: group.group('url') if group else None)\\\n",
    "    .map(lambda url: (url, 1))\\\n",
    "    .reduceByKey(lambda a,b:a+b)\\\n",
    "    .sortBy(lambda pair: -pair[1])\n",
    "# this orders by the second pair item, where pair is ('path', count)\n",
    "\n",
    "actual_counts = counts.collect()\n",
    "\n",
    "# The first 20 items:\n",
    "for path, count in actual_counts[:20]:\n",
    "    print(\"%s: %d\" % (path, count))\n",
    "    \n",
    "# Here you can see the None problem. To fix it a better regex needs to be created or another parsing method should be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce wordcount in Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_file = sc.textFile('hdfs://localhost:8020/user/root/gutenberg_total.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "sys.path.insert(0, './Portfolio')\n",
    "from MapReduce_code import allStopWords as stopwords\n",
    "\n",
    "punc = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "\n",
    "def rem_punctuation(inp):\n",
    "    return inp.translate(punc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one:\t49502\nsaid:\t49272\nnow:\t31991\nwill:\t28301\nus:\t28247\ntime:\t26309\nlike:\t26102\nproject:\t25751\ncan:\t24689\nback:\t24330\n"
     ]
    }
   ],
   "source": [
    "word_count = gutenberg_file.flatMap(lambda line: \n",
    "                                    [word for word in rem_punctuation(line).lower().split(' ')])\\\n",
    "    .filter(lambda word: len(word) > 1)\\\n",
    "    .filter(lambda word: word not in stopwords)\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .sortBy(lambda pair: -pair[1])\n",
    "\n",
    "actual_word_count = word_count.collect()\n",
    "\n",
    "for word, count in actual_word_count[:10]:\n",
    "    print(\"%s:\\t%d\" % (word, count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note to teacher\n",
    "- sc.textFile already returns a RDD object\n",
    "- map, flatMap already is an transformation\n",
    "- filter, reduceByKey, sortBy already is an action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
